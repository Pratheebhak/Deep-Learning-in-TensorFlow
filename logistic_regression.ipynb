{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Igm2cT0dVM"
      },
      "source": [
        "## Logistic Regression using Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVgSPtJjZILp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "d871eedf-e90b-401b-8e3f-00b124effaae"
      },
      "source": [
        "# Logistic regression using Adam Optimizer\n",
        "import os\n",
        "import sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tfe = tf.contrib.eager\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# import utils\n",
        "# Define paramaters for the model\n",
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "n_epochs = 25\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "# Step 1: Read in data\n",
        "fmnist_folder = 'data/fashion'\n",
        "#Create dataset load function [Refer fashion mnist github page for util function]\n",
        "#Create train,validation,test split\n",
        "data = input_data.read_data_sets(fmnist_folder, one_hot=True)\n",
        "# train, val, test = utils.read_fmnist(fmnist_folder, flatten=True)\n",
        "\n",
        "# Step 2: Create datasets and iterator\n",
        "# create training Dataset and batch it\n",
        "train_data = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32))).shuffle(buffer_size=1000).batch(batch_size)\n",
        "dataset_iter = tfe.Iterator(train_data)\n",
        "\n",
        "# create testing Dataset and batch it\n",
        "test_data = tf.data.Dataset.from_tensor_slices((data.test.images, data.test.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32))).shuffle(buffer_size=1000).batch(batch_size)\n",
        "\n",
        "\n",
        "# create one iterator and initialize it with different datasets\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
        "                                           train_data.output_shapes)\n",
        "#img, label = iterator.get_next()\n",
        "train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n",
        "test_init = iterator.make_initializer(test_data)\t# initializer for test_data\n",
        "\n",
        "# Step 3: create weights and bias\n",
        "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
        "# b is initialized to 0\n",
        "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
        "# shape of b depends on Y\n",
        "\n",
        "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "\n",
        "# Step 4: build model\n",
        "# the model that returns the logits.\n",
        "# this logits will be later passed through softmax layer\n",
        "\n",
        "def model_output(img):\n",
        "  model_output = tf.matmul(img, w) + b\n",
        "  return model_output\n",
        "\n",
        "# Step 5: define loss function\n",
        "# use cross entropy of softmax of logits as the loss function\n",
        "\n",
        "def loss(img, label):\n",
        "  logits = model_output(img)\n",
        "  cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
        "  return tf.reduce_mean(cross_entropy_loss)\n",
        "\n",
        "# Step 6: define optimizer\n",
        "# using Adam Optimizer with pre-defined learning rate to minimize loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "grad = tfe.implicit_gradients(loss)\n",
        "\n",
        "\n",
        "# Step 7: calculate accuracy with test set\n",
        "# preds = tf.nn.softmax(logits)\n",
        "# correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
        "# accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "# Step 8: train the model for n_epochs times\n",
        "\n",
        "total_loss=0.0\n",
        "batch_accuracy=0.0\n",
        "for i in range(n_epochs):\n",
        "    for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_data)):\n",
        "        total_loss = loss(image_batch, label_batch)\n",
        "        prediction = tf.nn.softmax(model_output(image_batch))\n",
        "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(label_batch, 1))\n",
        "        batch_accuracy += tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        if (step%1000 == 0):\n",
        "            print(\"epoch: {} batch_loss: {} batch_accuracy: {}\".format(i, total_loss.numpy(),batch_accuracy.numpy()))\n",
        "            total_loss=0.\n",
        "            batch_accuracy=0.\n",
        "        optimizer.apply_gradients(grad(image_batch, label_batch))\n",
        "\n",
        "#Step 9: Get the Final test accuracy\n",
        "prediction = tf.nn.softmax(model_output(data.test.images))\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(data.test.labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "print(\"Testset Accuracy: {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
            "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
            "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n",
            "epoch: 0 batch_loss: 2.3034355640411377 batch_accuracy: 0.140625\n",
            "epoch: 1 batch_loss: 2.7775068283081055 batch_accuracy: 743.8125\n",
            "epoch: 2 batch_loss: 3.2382421493530273 batch_accuracy: 759.390625\n",
            "epoch: 3 batch_loss: 2.5180447101593018 batch_accuracy: 762.2135620117188\n",
            "epoch: 4 batch_loss: 2.469020128250122 batch_accuracy: 764.9375\n",
            "epoch: 5 batch_loss: 2.536775588989258 batch_accuracy: 767.515625\n",
            "epoch: 6 batch_loss: 3.484461545944214 batch_accuracy: 768.359375\n",
            "epoch: 7 batch_loss: 3.2625207901000977 batch_accuracy: 767.9426879882812\n",
            "epoch: 8 batch_loss: 3.1447901725769043 batch_accuracy: 768.40625\n",
            "epoch: 9 batch_loss: 3.3887946605682373 batch_accuracy: 769.078125\n",
            "epoch: 10 batch_loss: 1.9695372581481934 batch_accuracy: 770.015625\n",
            "epoch: 11 batch_loss: 3.0409328937530518 batch_accuracy: 769.625\n",
            "epoch: 12 batch_loss: 3.442600727081299 batch_accuracy: 770.34375\n",
            "epoch: 13 batch_loss: 3.094108819961548 batch_accuracy: 770.78125\n",
            "epoch: 14 batch_loss: 2.3088784217834473 batch_accuracy: 772.1875\n",
            "epoch: 15 batch_loss: 3.1154701709747314 batch_accuracy: 771.375\n",
            "epoch: 16 batch_loss: 2.6558098793029785 batch_accuracy: 770.796875\n",
            "epoch: 17 batch_loss: 3.308495044708252 batch_accuracy: 771.6875\n",
            "epoch: 18 batch_loss: 2.522533655166626 batch_accuracy: 774.5625\n",
            "epoch: 19 batch_loss: 2.8858652114868164 batch_accuracy: 773.6926879882812\n",
            "epoch: 20 batch_loss: 3.1500892639160156 batch_accuracy: 772.71875\n",
            "epoch: 21 batch_loss: 2.916853427886963 batch_accuracy: 773.9375\n",
            "epoch: 22 batch_loss: 2.453284978866577 batch_accuracy: 773.84375\n",
            "epoch: 23 batch_loss: 2.6432149410247803 batch_accuracy: 773.5\n",
            "epoch: 24 batch_loss: 1.9971718788146973 batch_accuracy: 774.9375\n",
            "Testset Accuracy: 0.8645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQTw2lHl0rw6"
      },
      "source": [
        "## Logistic Regression using Gradient Descent Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6gRDtRSfj-S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "2f8ac775-d20c-4a8c-b071-4e85bf885e0f"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tfe = tf.contrib.eager\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# import utils\n",
        "# Define paramaters for the model\n",
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "n_epochs = 25\n",
        "n_train = 60000\n",
        "n_test = 10000\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "# Step 1: Read in data\n",
        "fmnist_folder = 'data/fashion'\n",
        "#Create dataset load function [Refer fashion mnist github page for util function]\n",
        "#Create train,validation,test split\n",
        "data = input_data.read_data_sets(fmnist_folder, one_hot=True)\n",
        "# train, val, test = utils.read_fmnist(fmnist_folder, flatten=True)\n",
        "\n",
        "# Step 2: Create datasets and iterator\n",
        "# create training Dataset and batch it\n",
        "train_data = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32))).shuffle(buffer_size=1000).batch(batch_size)\n",
        "dataset_iter = tfe.Iterator(train_data)\n",
        "\n",
        "# create testing Dataset and batch it\n",
        "test_data = tf.data.Dataset.from_tensor_slices((data.test.images, data.test.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32))).shuffle(buffer_size=1000).batch(batch_size)\n",
        "\n",
        "\n",
        "# create one iterator and initialize it with different datasets\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
        "                                           train_data.output_shapes)\n",
        "#img, label = iterator.get_next()\n",
        "train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n",
        "test_init = iterator.make_initializer(test_data)\t# initializer for test_data\n",
        "\n",
        "# Step 3: create weights and bias\n",
        "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
        "# b is initialized to 0\n",
        "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
        "# shape of b depends on Y\n",
        "\n",
        "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "\n",
        "# Step 4: build model\n",
        "# the model that returns the logits.\n",
        "# this logits will be later passed through softmax layer\n",
        "\n",
        "def model_output(img):\n",
        "  model_output = tf.matmul(img, w) + b\n",
        "  return model_output\n",
        "\n",
        "# Step 5: define loss function\n",
        "# use cross entropy of softmax of logits as the loss function\n",
        "\n",
        "def loss(img, label):\n",
        "  logits = model_output(img)\n",
        "  cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
        "  return tf.reduce_mean(cross_entropy_loss)\n",
        "\n",
        "# Step 6: define optimizer\n",
        "# using Adam Optimizer with pre-defined learning rate to minimize loss\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "grad = tfe.implicit_gradients(loss)\n",
        "\n",
        "\n",
        "# Step 7: calculate accuracy with test set\n",
        "# preds = tf.nn.softmax(logits)\n",
        "# correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
        "# accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "# Step 8: train the model for n_epochs times\n",
        "\n",
        "total_loss=0.0\n",
        "batch_accuracy=0.0\n",
        "for i in range(n_epochs):\n",
        "    for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_data)):\n",
        "        total_loss = loss(image_batch, label_batch)\n",
        "        prediction = tf.nn.softmax(model_output(image_batch))\n",
        "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(label_batch, 1))\n",
        "        batch_accuracy += tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        if (step%1000 == 0):\n",
        "            print(\"epoch: {} batch_loss: {} batch_accuracy: {}\".format(i, total_loss.numpy(),batch_accuracy.numpy()))\n",
        "            total_loss=0.\n",
        "            batch_accuracy=0.\n",
        "        optimizer.apply_gradients(grad(image_batch, label_batch))\n",
        "\n",
        "#Step 9: Get the Final test accuracy\n",
        "prediction = tf.nn.softmax(model_output(data.test.images))\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(data.test.labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "print(\"Testset Accuracy: {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
            "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
            "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n",
            "epoch: 0 batch_loss: 2.3143441677093506 batch_accuracy: 0.0625\n",
            "epoch: 1 batch_loss: 0.2831118404865265 batch_accuracy: 753.4583129882812\n",
            "epoch: 2 batch_loss: 0.23733747005462646 batch_accuracy: 778.0208129882812\n",
            "epoch: 3 batch_loss: 0.21846097707748413 batch_accuracy: 784.0676879882812\n",
            "epoch: 4 batch_loss: 0.20782425999641418 batch_accuracy: 787.1614379882812\n",
            "epoch: 5 batch_loss: 0.20083370804786682 batch_accuracy: 789.0989379882812\n",
            "epoch: 6 batch_loss: 0.1957944631576538 batch_accuracy: 790.1926879882812\n",
            "epoch: 7 batch_loss: 0.19193628430366516 batch_accuracy: 791.3176879882812\n",
            "epoch: 8 batch_loss: 0.18885937333106995 batch_accuracy: 792.0208129882812\n",
            "epoch: 9 batch_loss: 0.18633465468883514 batch_accuracy: 792.7864379882812\n",
            "epoch: 10 batch_loss: 0.18422015011310577 batch_accuracy: 793.6145629882812\n",
            "epoch: 11 batch_loss: 0.1824222207069397 batch_accuracy: 794.1458129882812\n",
            "epoch: 12 batch_loss: 0.1808754950761795 batch_accuracy: 794.8489379882812\n",
            "epoch: 13 batch_loss: 0.17953242361545563 batch_accuracy: 795.0208129882812\n",
            "epoch: 14 batch_loss: 0.1783572882413864 batch_accuracy: 795.2864379882812\n",
            "epoch: 15 batch_loss: 0.17732243239879608 batch_accuracy: 795.8645629882812\n",
            "epoch: 16 batch_loss: 0.17640598118305206 batch_accuracy: 796.3333129882812\n",
            "epoch: 17 batch_loss: 0.1755904257297516 batch_accuracy: 796.6614379882812\n",
            "epoch: 18 batch_loss: 0.17486125230789185 batch_accuracy: 797.0208129882812\n",
            "epoch: 19 batch_loss: 0.17420682311058044 batch_accuracy: 797.3176879882812\n",
            "epoch: 20 batch_loss: 0.17361712455749512 batch_accuracy: 797.6614379882812\n",
            "epoch: 21 batch_loss: 0.17308396100997925 batch_accuracy: 797.9114379882812\n",
            "epoch: 22 batch_loss: 0.17260020971298218 batch_accuracy: 797.8854370117188\n",
            "epoch: 23 batch_loss: 0.1721600592136383 batch_accuracy: 798.1198120117188\n",
            "epoch: 24 batch_loss: 0.17175835371017456 batch_accuracy: 798.2135620117188\n",
            "Testset Accuracy: 0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}