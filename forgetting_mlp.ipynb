{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forgetting_mlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLMTQNSN1D_d"
      },
      "source": [
        "## Measuring Catastrophic Forgetting in Multi-layer Perceptron\r\n",
        "\r\n",
        "**References:** \\\\\r\n",
        "[1] Lopez-Paz, D., et al. Gradient episodic memory for continual learning. In Advances in NeuralInformation Processing Systems (2017), pp. 6470â€“6479. \\\\\r\n",
        "[2] Ororbia, A., Mali, A., Kifer, D., and Giles, C. L. Lifelong neural predictive coding:  Sparsityyields less forgetting when learning cumulatively. CoRR abs/1905.10696 (2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s6NgveAkwHB"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "tf.enable_eager_execution()\n",
        "tf.executing_eagerly()\n",
        "\n",
        "\n",
        "#Unique Seed Definition\n",
        "tf.set_random_seed(11)\n",
        "np.random.seed(11)\n",
        "\n",
        "\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden: int, size of hidden layer\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "        \"\"\"\n",
        "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "        size_input, size_hidden, size_output, device\n",
        "    \n",
        "        # 2 layers\n",
        "        # Initialize weights between input layer and hidden layer\n",
        "        self.W1 = tf.Variable(tf.random_normal([self.size_input, self.size_hidden],stddev=0.1),name=\"W1\")\n",
        "        # Initialize biases for hidden layer\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden]), name = \"b1\")\n",
        "        # Initialize weights between hidden layer and output layer\n",
        "        self.W2 = tf.Variable(tf.random_normal([self.size_hidden, self.size_output],stddev=0.1),name=\"W2\")\n",
        "        # Initialize biases for output layer\n",
        "        self.b2 = tf.Variable(tf.random_normal([1, self.size_output]),name=\"b2\")\n",
        "            \n",
        "        # Define variables to be updated during backpropagation\n",
        "        self.variables = [self.W1, self.b1,self.W2, self.b2]\n",
        "        \n",
        "        # 3 layers        \n",
        "        #self.W1 = tf.Variable(tf.random_normal([self.size_input, self.size_hidden],stddev=0.1),name=\"W1\")\n",
        "        #self.b1 = tf.Variable(tf.zeros([1, self.size_hidden]), name = \"b1\")\n",
        "        #self.W2 = tf.Variable(tf.random_normal([self.size_hidden, self.size_hidden],stddev=0.1),name=\"W2\")\n",
        "        #self.b2 = tf.Variable(tf.random_normal([1, self.size_hidden]),name=\"b2\")\n",
        "        #self.W3 = tf.Variable(tf.random_normal([self.size_hidden, self.size_output],stddev=0.1),name=\"W3\")\n",
        "        #self.b3 = tf.Variable(tf.random_normal([1, self.size_output]),name=\"b3\")\n",
        "            \n",
        "        #self.variables = [self.W1, self.b1,self.W2, self.b2, self.W3, self.b3]\n",
        "        \n",
        "        # 4 layers        \n",
        "        #self.W1 = tf.Variable(tf.random_normal([self.size_input, self.size_hidden],stddev=0.1),name=\"W1\")\n",
        "        #self.b1 = tf.Variable(tf.zeros([1, self.size_hidden]), name = \"b1\")\n",
        "        #self.W2 = tf.Variable(tf.random_normal([self.size_hidden, self.size_hidden],stddev=0.1),name=\"W2\")\n",
        "        #self.b2 = tf.Variable(tf.random_normal([1, self.size_hidden]),name=\"b2\")\n",
        "        #self.W3 = tf.Variable(tf.random_normal([self.size_hidden, self.size_hidden],stddev=0.1),name=\"W3\")\n",
        "        #self.b3 = tf.Variable(tf.random_normal([1, self.size_hidden]),name=\"b3\")\n",
        "        #self.W4 = tf.Variable(tf.random_normal([self.size_hidden, self.size_output],stddev=0.1),name=\"W4\")\n",
        "        #self.b4 = tf.Variable(tf.random_normal([1, self.size_output]),name=\"b4\")\n",
        "            \n",
        "        #self.variables = [self.W1, self.b1,self.W2, self.b2, self.W3, self.b3, self.W4, self.b4]\n",
        "        \n",
        "    \n",
        "    # prediction\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward pass\n",
        "        X: Tensor, inputs\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "      \n",
        "        return self.y\n",
        "    \n",
        "    ## loss function\n",
        "    def loss(self, y_pred, y_true):\n",
        "        '''\n",
        "        y_pred - Tensor of shape (batch_size, size_output)\n",
        "        y_true - Tensor of shape (batch_size, size_output)\n",
        "        '''\n",
        "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        #loss1 = tf.losses.mean_squared_error(y_true_tf,y_pred_tf)\n",
        "        #loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
        "        #return tf.losses.mean_squared_error(y_true_tf,y_pred_tf)\n",
        "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\n",
        "        #return loss1 + loss2\n",
        "        \n",
        "  \n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        backward pass\n",
        "        \"\"\"\n",
        "        # optimizer\n",
        "        # Test with SGD,Adam, RMSProp\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        #optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        optimizer.apply_gradients(zip(grads, self.variables),\n",
        "                              global_step=tf.train.get_or_create_global_step())\n",
        "        \n",
        "        \n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to obtain output tensor during forward pass\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        #Remember to normalize your dataset before moving forward\n",
        "        # Compute values in hidden layer\n",
        "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        hhat = tf.nn.relu(what)\n",
        "        what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "        hhat1 = tf.nn.relu(what1)\n",
        "        #what2 = tf.matmul(hhat1, self.W3) + self.b3\n",
        "        #hhat2 = tf.nn.relu(what2)\n",
        "        #what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        #hhat = tf.nn.relu(what)\n",
        "        #Dropout\n",
        "        #hhat_tilda = tf.compat.v1.nn.dropout(hhat,rate=0.2)\n",
        "        # Compute output\n",
        "        output = tf.matmul(hhat, self.W2) + self.b2\n",
        "        #output = tf.matmul(hhat1, self.W3) + self.b3\n",
        "        #output = tf.matmul(hhat2, self.W4) + self.b4\n",
        "        #output = tf.matmul(hhat_tilda, self.W2) + self.b2\n",
        "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "        #Second add tf.Softmax(output) and then return this variable\n",
        "        #print(output)\n",
        "        return (output)\n",
        "        #return output\n",
        "        \n",
        "def accuracy_function(yhat,true_y):\n",
        "  correct_prediction = tf.equal(tf.argmax(yhat, 1), tf.argmax(true_y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  return accuracy\n",
        "  \n",
        "# Load MNIST Dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/\", one_hot = True)\n",
        "\n",
        "size_input = 784\n",
        "size_hidden = 256\n",
        "size_output = 10\n",
        "\n",
        "X_train = mnist.train.images\n",
        "y_train = mnist.train.labels\n",
        "X_test = mnist.test.images\n",
        "y_test = mnist.test.labels\n",
        "\n",
        "## Permuted MNIST\n",
        "\n",
        "num_tasks_to_run = 10\n",
        "num_epochs_per_task = 20\n",
        "\n",
        "# Generate the tasks specifications as a list of random permutations of the input pixels.\n",
        "task_permutation = []\n",
        "for task in range(num_tasks_to_run):\n",
        "\ttask_permutation.append(np.random.permutation(784))\n",
        "  \n",
        "minibatch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Resulting task Matrix\n",
        "R = np.zeros([10,10])\n",
        "\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).map(lambda x, y: (x, tf.cast(y, tf.float32))).batch(20)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test,y_test)).map(lambda x, y: (x, tf.cast(y, tf.float32))).batch(20)\n",
        "\n",
        "# Define metrics\n",
        "T = 9\n",
        "\n",
        "def acc(R):\n",
        "  acc = 0\n",
        "  for i in range(T):\n",
        "    acc += R[T][i]\n",
        "  return acc/T\n",
        "\n",
        "def bwt(R):\n",
        "  bwt = 0\n",
        "  for i in range(T-1):\n",
        "    bwt += R[T][i] - R[i][i]\n",
        "  return bwt/(T - 1)\n",
        "\n",
        "def cbwt(R):\n",
        "  cbwt = []\n",
        "  ans = 0\n",
        "  for t in range(T):\n",
        "    sum = 0\n",
        "    for i in range(t+1,T):\n",
        "      sum += R[T][i] - R[t][t]\n",
        "      cbwt.append(sum/(T-t))\n",
        "      ans += sum/(T-t)\n",
        "  return ans/10\n",
        "  \n",
        "G = np.random.randn(10,10)\n",
        "def tbwt(R,G):\n",
        "  tbwt = 0\n",
        "  for i in range(1,T-1):\n",
        "    tbwt+= R[T][i] - G[i][i]\n",
        "  return tbwt/(T-1)\n",
        "\n",
        "# Training and Testing\n",
        "model = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "for train_task in range(num_tasks_to_run):\n",
        "  print(\"Training Task: {}\".format(train_task + 1))\n",
        "  pmnist_train = X_train[:,task_permutation[train_task]]\n",
        "  num_train = 55000\n",
        "  for epoch in range(num_epochs_per_task):\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((pmnist_train, y_train)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
        "           .shuffle(buffer_size=1000)\\\n",
        "           .batch(batch_size=minibatch_size)\n",
        "    loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = model.forward(inputs)\n",
        "      loss_total = loss_total + model.loss(preds, outputs)\n",
        "      model.backward(inputs, outputs)\n",
        "    preds_val = model.compute_output(mnist.validation.images)\n",
        "    accuracy_val = accuracy_function(preds_val,mnist.validation.labels)\n",
        "    accuracy_val = accuracy_val * 100\n",
        "    print (\"Validation Accuracy = {}\".format(accuracy_val.numpy()))\n",
        "  for test_task in range(num_tasks_to_run):\n",
        "    pmnist_test = X_test[:,task_permutation[test_task]]\n",
        "    preds_test = model.compute_output(pmnist_test)\n",
        "    accuracy_test = accuracy_function(preds_test,y_test)\n",
        "    R[train_task][test_task] = accuracy_test\n",
        "    print('Test Accuracy on Task:{} is {:.4f}'.format(test_task+1, accuracy_test.numpy()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxvQKxaY1CmU"
      },
      "source": [
        "print('Resulting Task Matrix R {}'.format(R))\n",
        "\n",
        "print('Average Accuracy ACC {:.4f}'.format(acc(R)))\n",
        "print('Backward Transfer BWT {:.4f}'.format(bwt(R)))\n",
        "print('Cumulative Backward Transfer CBWT {:.4f}'.format(cbwt(R)))\n",
        "print('True Backward Transfer TBWT {:.4f}'.format(tbwt(R,G)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}