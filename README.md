# Deep-Learning-in-TensorFlow
Implementation of Machine Learning and Deep Learning Algorithms from scratch in Tensorflow

## Contents

1. **[Linear Regression](https://github.com/Pratheebhak/Deep-Learning-in-TensorFlow/blob/main/linear_regression.ipynb)**
    * L2 Loss function
    * L1 Loss function
    * Hybrid Loss function
2. **[Logistic Regression](https://github.com/Pratheebhak/Deep-Learning-in-TensorFlow/blob/main/logistic_regression.ipynb)**
    * Adam Optimizer
    * Gradient Descent Optimizer
3. **[Forgetting MLP](https://github.com/Pratheebhak/Deep-Learning-in-TensorFlow/blob/main/forgetting_mlp.ipynb)**
    * Measuring catastrophic forgetting in Multi-layer perceptron
    * Catastrophic forgetting is when learning new tasks hurts the performance of the learner at previously solved tasks
4. **[Convolutional Neural Network](https://github.com/Pratheebhak/Deep-Learning-in-TensorFlow/blob/main/cnn.ipynb)**
    * Batch Normalization
    * Weight Normalization
    * Layer Normaliation
5. **[Recurrent Neural Networks](https://github.com/Pratheebhak/Deep-Learning-in-TensorFlow/blob/main/rnn.ipynb)**
    * Long Short-Term Memory Network (LSTM)
    * Gated Recurrent Unit (GRU)
    * Minimal Gated Unit (MGU)



## References

* Lopez-Paz, D., et al. Gradient episodic memory for continual learning. In Advances in NeuralInformation Processing Systems (2017), pp. 6470–6479
* Ororbia, A., Mali, A., Kifer, D., and Giles, C. L. Lifelong neural predictive coding:  Sparsityyields less forgetting when learning cumulatively. CoRR abs/1905.10696 (2019)
* Ioffe, S., and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internalcovariate shift. arXiv preprint arXiv:1502.03167 (2015)
* Lei Ba, J., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450 (2016)
* Salimans, T., and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training ofdeep neural networks. In Advances in Neural Information Processing Systems (2016), pp. 901– 909
* Cho, K., Van Merrienboer, B., Bahdanau, D., and Bengio, Y.  On the properties of neural machine translation:Encoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014)
* Zhou,  G.,  Wu,  J.,  Zhang,  C.,  and Zhou,  Z. Minimal gated unit for recurrent neural networks.   CoRRabs/1603.09420 (2016).
