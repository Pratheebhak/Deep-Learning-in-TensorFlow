{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0EwXvQe172z"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import tensorflow.contrib.eager as tfe\r\n",
        "tf.enable_eager_execution()\r\n",
        "tf.executing_eagerly()\r\n",
        "seed = 1234\r\n",
        "tf.random.set_random_seed(seed=seed)\r\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13nqcXee1-jR"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\r\n",
        "data = input_data.read_data_sets(\"/tmp/data/fashion\", one_hot=True, reshape=False)\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "hidden_size = 100\r\n",
        "learning_rate = 0.01\r\n",
        "output_size = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVXxLOSX2CKV"
      },
      "source": [
        "class CNN(object):\r\n",
        "  def __init__(self,hidden_size,output_size,device=None):\r\n",
        "      filter_h, filter_w, filter_c , filter_n = 5 ,5 ,1 ,30\r\n",
        "      self.W1 = tf.Variable(tf.random_normal([filter_h, filter_w, filter_c, filter_n], stddev=0.1))\r\n",
        "      self.b1 = tf.Variable(tf.zeros([filter_n]),dtype=tf.float32)\r\n",
        "      self.W2 = tf.Variable(tf.random_normal([14*14*filter_n, hidden_size], stddev=0.1))\r\n",
        "      self.b2 = tf.Variable(tf.zeros([hidden_size]),dtype=tf.float32)\r\n",
        "      self.W3 = tf.Variable(tf.random_normal([hidden_size, output_size], stddev=0.1))\r\n",
        "      self.b3 = tf.Variable(tf.zeros([output_size]),dtype=tf.float32)\r\n",
        "      # Weight Normalization Parameters\r\n",
        "      self.g = tf.Variable(1,dtype = tf.float32)\r\n",
        "      self.v = tf.Variable(tf.random_normal([filter_h, filter_w, filter_c, filter_n], stddev=0.05))\r\n",
        "      # Batch, Layer Normalization Parameters\r\n",
        "      self.gamma = tf.Variable(tf.ones([filter_n]),  dtype = tf.float32)\r\n",
        "      self.beta = tf.Variable(0,dtype = tf.float32)\r\n",
        "      \r\n",
        "      self.variables = [self.W1,self.b1, self.W2, self.b2, self.W3, self.b3, self.g, self.v, self.gamma, self.beta]\r\n",
        "      self.device = device\r\n",
        "      self.size_output = output_size\r\n",
        "  \r\n",
        "  def flatten(self,X, window_h, window_w, window_c, out_h, out_w, stride=1, padding=0):\r\n",
        "    \r\n",
        "      X_padded = tf.pad(X, [[0,0], [padding, padding], [padding, padding], [0,0]])\r\n",
        "\r\n",
        "      windows = []\r\n",
        "      for y in range(out_h):\r\n",
        "          for x in range(out_w):\r\n",
        "              window = tf.slice(X_padded, [0, y*stride, x*stride, 0], [-1, window_h, window_w, -1])\r\n",
        "              windows.append(window)\r\n",
        "      stacked = tf.stack(windows) # shape : [out_h, out_w, n, filter_h, filter_w, c]\r\n",
        "\r\n",
        "      return tf.reshape(stacked, [-1, window_c*window_w*window_h])\r\n",
        "  \r\n",
        "  def convolution(self,X, W, b, padding, stride):\r\n",
        "      n, h, w, c = map(lambda d: d.value, X.get_shape())\r\n",
        "      #print(X.get_shape())\r\n",
        "      #print(data.train.images.get_shape())\r\n",
        "      filter_h, filter_w, filter_c, filter_n = [d.value for d in W.get_shape()]\r\n",
        "    \r\n",
        "      out_h = (h + 2*padding - filter_h)//stride + 1\r\n",
        "      out_w = (w + 2*padding - filter_w)//stride + 1\r\n",
        "\r\n",
        "      X_flat = self.flatten(X, filter_h, filter_w, filter_c, out_h, out_w, stride, padding)\r\n",
        "      W_flat = tf.reshape(W, [filter_h*filter_w*filter_c, filter_n])\r\n",
        "    \r\n",
        "      z = tf.matmul(X_flat, W_flat) + b     # b: 1 X filter_n\r\n",
        "    \r\n",
        "      return tf.transpose(tf.reshape(z, [out_h, out_w, n, filter_n]), [2, 0, 1, 3])\r\n",
        "  \r\n",
        "  ####################### Normalization Functions ##############################\r\n",
        "  \r\n",
        "  def BatchNorm(self, X, gamma, beta, eps):\r\n",
        "      mu_batch, sigma_batch = tf.nn.moments(X, axes = [0], keepdims = True)\r\n",
        "      x_hat = (X - mu_batch)/tf.sqrt(sigma_batch + eps)\r\n",
        "      return (self.gamma * x_hat) + self.beta\r\n",
        "\r\n",
        "  def LayerNorm(self, X, gamma, beta, eps):\r\n",
        "      mu_layer, sigma_layer = tf.nn.moments(X, axes = [1], keepdims = True)\r\n",
        "      x_hat = (X - mu_layer)/tf.sqrt(sigma_layer + eps)\r\n",
        "      return (self.gamma * x_hat) + self.beta\r\n",
        "  \r\n",
        "  def WeightNorm(self, v, g):\r\n",
        "      norm_v = tf.nn.l2_normalize(self.v,[0,1,2])\r\n",
        "      #norm_v = tf.norm(self.v, ord = 'euclidean', keepdims = True)\r\n",
        "      return (self.g/norm_v) * self.v \r\n",
        " ###############################################################################\r\n",
        "\r\n",
        "  def relu(self,X):\r\n",
        "      return tf.maximum(X, tf.zeros_like(X))\r\n",
        "    \r\n",
        "  def max_pool(self,X, pool_h, pool_w, padding, stride):\r\n",
        "      n, h, w, c = [d.value for d in X.get_shape()]\r\n",
        "    \r\n",
        "      out_h = (h + 2*padding - pool_h)//stride + 1\r\n",
        "      out_w = (w + 2*padding - pool_w)//stride + 1\r\n",
        "\r\n",
        "      X_flat = self.flatten(X, pool_h, pool_w, c, out_h, out_w, stride, padding)\r\n",
        "\r\n",
        "      pool = tf.reduce_max(tf.reshape(X_flat, [out_h, out_w, n, pool_h*pool_w, c]), axis=3)\r\n",
        "      return tf.transpose(pool, [2, 0, 1, 3])\r\n",
        " \r\n",
        "  def affine(self,X, W, b):\r\n",
        "      n = X.get_shape()[0].value # number of samples\r\n",
        "      X_flat = tf.reshape(X, [n, -1])\r\n",
        "      return tf.matmul(X_flat, W) + b \r\n",
        "    \r\n",
        "  def softmax(self,X):\r\n",
        "      X_centered = X - tf.reduce_max(X) # to avoid overflow\r\n",
        "      X_exp = tf.exp(X_centered)\r\n",
        "      exp_sum = tf.reduce_sum(X_exp, axis=1)\r\n",
        "      return tf.transpose(tf.transpose(X_exp) / exp_sum) \r\n",
        "    \r\n",
        "  def cross_entropy_error(self,yhat, y):\r\n",
        "      return -tf.reduce_mean(tf.log(tf.reduce_sum(yhat * y, axis=1)))\r\n",
        "    \r\n",
        "  def forward(self,X):\r\n",
        "      if self.device is not None:\r\n",
        "        with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\r\n",
        "          self.y = self.compute_output(X)\r\n",
        "      else:\r\n",
        "        self.y = self.compute_output(X)\r\n",
        "      \r\n",
        "      return self.y\r\n",
        "    \r\n",
        "  def loss(self, y_pred, y_true):\r\n",
        "      '''\r\n",
        "      y_pred - Tensor of shape (batch_size, size_output)\r\n",
        "      y_true - Tensor of shape (batch_size, size_output)\r\n",
        "      '''\r\n",
        "      y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\r\n",
        "      y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\r\n",
        "      return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=y_true_tf))\r\n",
        "    \r\n",
        "    \r\n",
        "  def backward(self, X_train, y_train):\r\n",
        "      \"\"\"\r\n",
        "      backward pass\r\n",
        "      \"\"\"\r\n",
        "      # optimizer\r\n",
        "      # Test with SGD,Adam, RMSProp\r\n",
        "      optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\r\n",
        "      #predicted = self.forward(X_train)\r\n",
        "      #current_loss = self.loss(predicted, y_train)\r\n",
        "      #optimizer.minimize(current_loss, self.variables)\r\n",
        "\r\n",
        "      #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n",
        "      with tf.GradientTape() as tape:\r\n",
        "          predicted = self.forward(X_train)\r\n",
        "          current_loss = self.loss(predicted, y_train)\r\n",
        "      #print(predicted)\r\n",
        "      #print(current_loss)\r\n",
        "      #current_loss_tf = tf.cast(current_loss, dtype=tf.float32)\r\n",
        "      grads = tape.gradient(current_loss, self.variables)\r\n",
        "      optimizer.apply_gradients(zip(grads, self.variables),\r\n",
        "                              global_step=tf.train.get_or_create_global_step())\r\n",
        "      \r\n",
        "      \r\n",
        "  def compute_output(self,X):\r\n",
        "\r\n",
        "    # Uncomment the block of codes to be run for each normalization\r\n",
        "      ## Without Normalization\r\n",
        "      #conv_layer1 = self.convolution(X, self.W1, self.b1, padding=2, stride=1)\r\n",
        "      #conv_activation = self.relu(conv_layer1)\r\n",
        "      \r\n",
        "      ## Weight Normalization - uncomment the given block of codes\r\n",
        "      weights = self.WeightNorm(self.v, self.g)\r\n",
        "      conv_layer1 = self.convolution(X, weights, self.b1, padding=2, stride=1)\r\n",
        "      conv_activation = self.relu(conv_layer1)\r\n",
        "    \r\n",
        "      ## Batch Normalization - my function\r\n",
        "      #conv_layer1 = self.convolution(X, self.W1, self.b1, padding=2, stride=1)\r\n",
        "      #conv_norm = self.BatchNorm(conv_layer1, self.gamma, self.beta, eps = 0.001)\r\n",
        "      #conv_activation = self.relu(conv_norm)\r\n",
        "\r\n",
        "      ## Batch Normalization - tensorflow function\r\n",
        "      #conv_layer1 = self.convolution(X, self.W1, self.b1, padding=2, stride=1)\r\n",
        "      #conv_norm = tf.contrib.layers.batch_norm(conv_layer1)\r\n",
        "      #conv_activation = self.relu(conv_norm)\r\n",
        "\r\n",
        "      ## Layer Normalization - my function\r\n",
        "      #conv_layer1 = self.convolution(X, self.W1, self.b1, padding=2, stride=1)\r\n",
        "      #conv_norm = self.LayerNorm(conv_layer1, self.gamma, self.beta, eps = 0.001)\r\n",
        "      #conv_activation = self.relu(conv_norm)\r\n",
        "      \r\n",
        "      ## Layer Normalization - tensorflow function\r\n",
        "      #conv_layer1 = self.convolution(X, self.W1, self.b1, padding=2, stride=1)\r\n",
        "      #conv_norm = tf.contrib.layers.layer_norm(conv_layer1)\r\n",
        "      #conv_activation = self.relu(conv_norm)      \r\n",
        "      \r\n",
        "      conv_pool = self.max_pool(conv_activation, pool_h=2, pool_w=2, padding=0, stride=2)\r\n",
        "      conv_affine =self.affine(conv_pool, self.W2,self.b2)\r\n",
        "      conv_affine_activation = self.relu(conv_affine)\r\n",
        "      \r\n",
        "      conv_affine_1 = self.affine(conv_affine_activation, self.W3, self.b3)\r\n",
        "      return conv_affine_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Wu1C632SJW"
      },
      "source": [
        "def accuracy_function(yhat,true_y):\r\n",
        "  correct_prediction = tf.equal(tf.argmax(yhat, 1), tf.argmax(true_y, 1))\r\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-LYqNS01yI9"
      },
      "source": [
        "# Initialize model using GPU\r\n",
        "mlp_on_cpu = CNN(hidden_size,output_size, device='gpu')\r\n",
        "\r\n",
        "num_epochs = 4\r\n",
        "train_x =  tf.convert_to_tensor(data.train.images)\r\n",
        "train_y = tf.convert_to_tensor(data.train.labels)\r\n",
        "time_start = time.time()\r\n",
        "num_train = 55000\r\n",
        "z= 0\r\n",
        "\r\n",
        "idx = [0]\r\n",
        "for set in range(10):\r\n",
        "  idx.append(5500*(set+1))\r\n",
        "\r\n",
        "for subset in range(10):\r\n",
        "  train_images = train_x[idx[subset-1]:idx[subset]]\r\n",
        "  train_labels = train_y[idx[subset-1]:idx[subset]]\r\n",
        "  accuracy = []\r\n",
        "  for epoch in range(num_epochs):\r\n",
        "      train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\r\n",
        "           .shuffle(buffer_size=1000)\\\r\n",
        "           .batch(batch_size=batch_size)\r\n",
        "      loss_total = tf.Variable(0, dtype=tf.float32)\r\n",
        "      accuracy_total = tf.Variable(0, dtype=tf.float32)\r\n",
        "      for inputs, outputs in train_ds:\r\n",
        "        preds = mlp_on_cpu.forward(inputs)\r\n",
        "        loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\r\n",
        "#             accuracy_train = accuracy_function(preds,outputs)\r\n",
        "#             accuracy_total = accuracy_total + accuracy_train\r\n",
        "        mlp_on_cpu.backward(inputs, outputs)\r\n",
        "            #print(z)\r\n",
        "            #z = z+ 1\r\n",
        "        #print('Number of Epoch = {} - loss:= {:.4f}'.format(epoch + 1, loss_total.numpy() / num_train))\r\n",
        "      preds = mlp_on_cpu.compute_output(train_images)\r\n",
        "      accuracy_train = accuracy_function(preds,train_labels)\r\n",
        "      accuracy.append(accuracy_train * 100)\r\n",
        "      \r\n",
        "acc = np.mean(accuracy)\r\n",
        "print(\"Training Accuracy = {}\".format(acc))        \r\n",
        "        \r\n",
        "# validation accuracy\r\n",
        "preds_val = mlp_on_cpu.compute_output(data.validation.images)\r\n",
        "accuracy_val = accuracy_function(preds_val,data.validation.labels)\r\n",
        "accuracy_val = accuracy_val * 100\r\n",
        "print (\"Validation Accuracy = {}\".format(accuracy_val.numpy()))\r\n",
        " \r\n",
        "# test accuracy\r\n",
        "test_x =  tf.convert_to_tensor(data.test.images)\r\n",
        "test_y = tf.convert_to_tensor(data.test.labels)\r\n",
        "preds_test = mlp_on_cpu.compute_output(test_x)\r\n",
        "accuracy_test = accuracy_function(preds_test,test_y)\r\n",
        "# To keep sizes compatible with model\r\n",
        "accuracy_test = accuracy_test * 100\r\n",
        "print (\"Test Accuracy = {}\".format(accuracy_test.numpy()))\r\n",
        "\r\n",
        "time_taken = time.time() - time_start\r\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\r\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}