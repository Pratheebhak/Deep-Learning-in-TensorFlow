{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXqjhKOS2zNh"
      },
      "source": [
        "# To unzip notMNIST data folder, make sure the directory in which the code is run has the notMNIST_small.zip file\r\n",
        "from zipfile import *\r\n",
        "zip = ZipFile('/content/notMNIST_small.zip')\r\n",
        "zip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36qMs2EC23Un"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from PIL import Image\r\n",
        "import os\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "#from tensorflow.python.keras.datasets import mnist\r\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\r\n",
        "from tensorflow.contrib.eager.python import tfe\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4opmFYd28vx"
      },
      "source": [
        "######################################### notMNIST Data ###############################################\r\n",
        "class notMNIST:\r\n",
        "    def __init__(self):\r\n",
        "        images, labels = [], []\r\n",
        "\r\n",
        "        for i, letter in enumerate(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']):\r\n",
        "            directory = 'notMNIST_small/%s/' % letter\r\n",
        "            files = os.listdir(directory)\r\n",
        "            label = np.array([0]*10)\r\n",
        "            label[i] = 1\r\n",
        "            for file in files:\r\n",
        "                try:\r\n",
        "                    im = Image.open(directory+file)\r\n",
        "                except:\r\n",
        "                    #print (\"Skip a corrupted file: \" + file)\r\n",
        "                    continue\r\n",
        "                pixels = np.array(im.convert('L').getdata())\r\n",
        "                images.append(pixels/255.0)\r\n",
        "                labels.append(label)\r\n",
        "          \r\n",
        "        train_images, test_images, train_labels, test_labels =  train_test_split(images, labels, test_size=0.2, random_state=0)\r\n",
        "        \r\n",
        "        class train:\r\n",
        "            def __init__(self):\r\n",
        "                self.images = []\r\n",
        "                self.labels = []\r\n",
        "                self.batch_counter = 0\r\n",
        "                \r\n",
        "            def next_batch(self, num):\r\n",
        "                if self.batch_counter + num >= len(self.labels):\r\n",
        "                    batch_images = self.images[self.batch_counter:]\r\n",
        "                    batch_labels = self.labels[self.batch_counter:]\r\n",
        "                    left = num - len(batch_labels)\r\n",
        "                    batch_images.extend(self.images[:left])\r\n",
        "                    batch_labels.extend(self.labels[:left])\r\n",
        "                    self.batch_counter = left\r\n",
        "                else:\r\n",
        "                    batch_images = self.images[self.batch_counter:self.batch_counter+num]\r\n",
        "                    batch_labels = self.labels[self.batch_counter:self.batch_counter+num]                  \r\n",
        "                    self.batch_counter += num\r\n",
        "                    \r\n",
        "                return (batch_images, batch_labels)\r\n",
        "                    \r\n",
        "        class test:\r\n",
        "            def __init__(self):\r\n",
        "                self.images = []\r\n",
        "                self.labels = []\r\n",
        "                \r\n",
        "        self.train = train()\r\n",
        "        self.test = test()\r\n",
        "                \r\n",
        "        self.train.images = train_images\r\n",
        "        self.train.labels = train_labels\r\n",
        "        self.test.images = test_images\r\n",
        "        self.test.labels = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68WJSHc33Xkz"
      },
      "source": [
        "\"\"\"\r\n",
        "Routine to create RNN Cells in Tensorflow 2.0 using eager execution.\r\n",
        "Code adapted from Google AI Language Team\r\n",
        "\"\"\"\r\n",
        "tf.enable_eager_execution()\r\n",
        "tf.set_random_seed(0)\r\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceUsz5nC3PFE"
      },
      "source": [
        "######################################### LSTM Model ###############################################\r\n",
        "class BasicLSTM(tf.keras.Model):\r\n",
        "    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):\r\n",
        "        super(BasicLSTM, self).__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.return_sequence = return_sequence\r\n",
        "        self.return_states = return_states\r\n",
        "\r\n",
        "        def bias_initializer(_, *args, **kwargs):\r\n",
        "            # Unit forget bias from the paper\r\n",
        "            # - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\r\n",
        "            return tf.keras.backend.concatenate([\r\n",
        "                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs),  # input gate\r\n",
        "                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  # forget gate\r\n",
        "                tf.keras.initializers.Zeros()((self.units * 2,), *args, **kwargs),  # context and output gates\r\n",
        "            ])\r\n",
        "\r\n",
        "        self.kernel = tf.keras.layers.Dense(4 * units, use_bias=False)\r\n",
        "        self.recurrent_kernel = tf.keras.layers.Dense(4 * units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)\r\n",
        "\r\n",
        "    def call(self, inputs, training=None, mask=None, initial_states=None):\r\n",
        "        # LSTM Cell in pure TF Eager code\r\n",
        "        # reset the states initially if not provided, else use those\r\n",
        "        if initial_states is None:\r\n",
        "            h_state = tf.zeros((inputs.shape[0], self.units))\r\n",
        "            c_state = tf.zeros((inputs.shape[0], self.units))\r\n",
        "        else:\r\n",
        "            assert len(initial_states) == 2, \"Must pass a list of 2 states when passing 'initial_states'\"\r\n",
        "            h_state, c_state = initial_states\r\n",
        "\r\n",
        "        h_list = []\r\n",
        "        c_list = []\r\n",
        "\r\n",
        "        for t in range(inputs.shape[1]):\r\n",
        "            # LSTM gate steps\r\n",
        "            ip = inputs[:, t, :]\r\n",
        "            z = self.kernel(ip)\r\n",
        "            z += self.recurrent_kernel(h_state)\r\n",
        "\r\n",
        "            z0 = z[:, :self.units]\r\n",
        "            z1 = z[:, self.units: 2 * self.units]\r\n",
        "            z2 = z[:, 2 * self.units: 3 * self.units]\r\n",
        "            z3 = z[:, 3 * self.units:]\r\n",
        "\r\n",
        "            # gate updates\r\n",
        "            i = tf.keras.activations.sigmoid(z0)\r\n",
        "            f = tf.keras.activations.sigmoid(z1)\r\n",
        "            c = f * c_state + i * tf.nn.tanh(z2)\r\n",
        "\r\n",
        "            # state updates\r\n",
        "            o = tf.keras.activations.sigmoid(z3)\r\n",
        "            h = o * tf.nn.tanh(c)\r\n",
        "\r\n",
        "            h_state = h\r\n",
        "            c_state = c\r\n",
        "\r\n",
        "            h_list.append(h_state)\r\n",
        "            c_list.append(c_state)\r\n",
        "\r\n",
        "        hidden_outputs = tf.stack(h_list, axis=1)\r\n",
        "        hidden_states = tf.stack(c_list, axis=1)\r\n",
        "\r\n",
        "        if self.return_states and self.return_sequence:\r\n",
        "            return hidden_outputs, [hidden_outputs, hidden_states]\r\n",
        "        elif self.return_states and not self.return_sequence:\r\n",
        "            return hidden_outputs[:, -1, :], [h_state, c_state]\r\n",
        "        elif self.return_sequence and not self.return_states:\r\n",
        "            return hidden_outputs\r\n",
        "        else:\r\n",
        "            return hidden_outputs[:, -1, :]\r\n",
        "\r\n",
        "class LSTM(tf.keras.Model):\r\n",
        "    def __init__(self, num_units, num_classes):\r\n",
        "        super(LSTM, self).__init__()\r\n",
        "        self.units = num_units\r\n",
        "        self.LSTM = BasicLSTM(num_units)\r\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes)\r\n",
        "    \r\n",
        "    def call(self, inputs, training = None, mask = None):\r\n",
        "        h = self.LSTM(inputs)\r\n",
        "        output = self.classifier(h)\r\n",
        "        \r\n",
        "        with tf.device('/cpu:0'):\r\n",
        "            output =tf.nn.softmax(output)\r\n",
        "        \r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cksAmiXH3fSu"
      },
      "source": [
        "######################################### GRU Model ###############################################\r\n",
        "class BasicGRU(tf.keras.Model):\r\n",
        "    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):\r\n",
        "        super(BasicGRU, self).__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.return_sequence = return_sequence\r\n",
        "        self.return_states = return_states\r\n",
        "\r\n",
        "        def bias_initializer(_, *args, **kwargs):\r\n",
        "            # Unit forget bias from the paper\r\n",
        "            # - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\r\n",
        "            return tf.keras.backend.concatenate([\r\n",
        "                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs),  # input gate\r\n",
        "                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  # forget gate\r\n",
        "                tf.keras.initializers.Zeros()((self.units * 2,), *args, **kwargs),  # context and output gates\r\n",
        "            ])\r\n",
        "\r\n",
        "        self.kernel = tf.keras.layers.Dense(4 * units, use_bias=False)\r\n",
        "        self.recurrent_kernel = tf.keras.layers.Dense(4 * units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)\r\n",
        "\r\n",
        "    def call(self, inputs, training=None, mask=None, initial_states=None):\r\n",
        "        # LSTM Cell in pure TF Eager code\r\n",
        "        # reset the states initially if not provided, else use those\r\n",
        "        if initial_states is None:\r\n",
        "            h_state = tf.zeros((inputs.shape[0], self.units))\r\n",
        "            #c_state = tf.zeros((inputs.shape[0], self.units))\r\n",
        "        else:\r\n",
        "            assert len(initial_states) == 2, \"Must pass a list of 2 states when passing 'initial_states'\"\r\n",
        "            h_state = initial_states\r\n",
        "\r\n",
        "        h_list = []\r\n",
        "        #c_list = []\r\n",
        "\r\n",
        "        for t in range(inputs.shape[1]):\r\n",
        "            \r\n",
        "            ip = inputs[:, t, :]\r\n",
        "            z = self.kernel(ip)\r\n",
        "            #z += self.recurrent_kernel(h_state)\r\n",
        "\r\n",
        "            z0 = z[:, :self.units]\r\n",
        "            z1 = z[:, self.units: 2 * self.units]\r\n",
        "            z2 = z[:, 2 * self.units: 3 * self.units]\r\n",
        "            \r\n",
        "            z_1 = self.recurrent_kernel(h_state)\r\n",
        "            z0 += z_1[:, :self.units]\r\n",
        "            z1 += z[:, self.units: 2 * self.units]\r\n",
        "\r\n",
        "            zt = tf.keras.activations.sigmoid(z0)\r\n",
        "            r = tf.keras.activations.sigmoid(z1)\r\n",
        "\r\n",
        "            z_2 = self.recurrent_kernel(h_state * r)\r\n",
        "            z2 += z_2[:, 2 * self.units: 3 * self.units]\r\n",
        "\r\n",
        "            s_tilde = tf.nn.tanh(z2)\r\n",
        "            h = (1 - zt) * h_state + (zt * s_tilde)\r\n",
        "\r\n",
        "            h_state = h\r\n",
        "            #c_state = c\r\n",
        "\r\n",
        "            h_list.append(h_state)\r\n",
        "            #c_list.append(c_state)\r\n",
        "\r\n",
        "        hidden_outputs = tf.stack(h_list, axis=1)\r\n",
        "        #hidden_states = tf.stack(c_list, axis=1)\r\n",
        "\r\n",
        "        return hidden_outputs[:, -1, :]\r\n",
        "\r\n",
        "class GRU(tf.keras.Model):\r\n",
        "    def __init__(self, num_units, num_classes):\r\n",
        "        super(GRU, self).__init__()\r\n",
        "        self.units = num_units\r\n",
        "        self.gru = BasicGRU(num_units)\r\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes)\r\n",
        "    \r\n",
        "    def call(self, inputs, training = None, mask = None):\r\n",
        "        h = self.gru(inputs)\r\n",
        "        output = self.classifier(h)\r\n",
        "        \r\n",
        "        with tf.device('/cpu:0'):\r\n",
        "            output =tf.nn.softmax(output)\r\n",
        "        \r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYVaEown3oxi"
      },
      "source": [
        "######################################### MGU Model ###############################################\r\n",
        "class BasicMGU(tf.keras.Model):\r\n",
        "    def __init__(self,num_hidden, return_sequence=False, return_states=False, **kwargs):\r\n",
        "        super(BasicMGU, self).__init__(**kwargs)\r\n",
        "        self.units            = num_hidden\r\n",
        "        self.return_sequence  = return_sequence\r\n",
        "        self.return_states    = return_states\r\n",
        "\r\n",
        "        def bias_initializer(_, *args, **kwargs):\r\n",
        "            return tf.keras.backend.concatenate([\r\n",
        "                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs), \r\n",
        "                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  \r\n",
        "            ])\r\n",
        "\r\n",
        "        self.kernel           = tf.keras.layers.Dense(2 * num_hidden, use_bias=False)\r\n",
        "        self.recurrent_kernel = tf.keras.layers.Dense(2 * num_hidden, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)\r\n",
        "\r\n",
        "    def call(self, inputs, training=None, mask=None, initial_states=None):\r\n",
        "        if initial_states is None:\r\n",
        "            h_state = tf.zeros((inputs.shape[0], self.units))\r\n",
        "        else:\r\n",
        "            assert len(initial_states) == 2, \"Must pass a list of 2 states when passing 'initial_states'\"\r\n",
        "            h_state = initial_states\r\n",
        "\r\n",
        "        h_list = []\r\n",
        "        #print('hello')\r\n",
        "        for t in range(inputs.shape[1]):\r\n",
        "            \r\n",
        "            ip = inputs[:, t, :]\r\n",
        "            z = self.kernel(ip)\r\n",
        "            z0 = z[:, :self.units]\r\n",
        "            z1 = z[:, self.units: 2 * self.units]\r\n",
        "      \r\n",
        "            z_0 = self.recurrent_kernel(h_state)\r\n",
        "            z0 += z_0[:, :self.units]\r\n",
        "  \r\n",
        "            f = tf.keras.activations.sigmoid(z0)\r\n",
        "\r\n",
        "            z_1 = self.recurrent_kernel(h_state * f)\r\n",
        "            z1 += z_1[:, self.units: 2 * self.units]\r\n",
        "\r\n",
        "            stilde = tf.nn.tanh(z1)\r\n",
        "\r\n",
        "            h = ((1 - f) * h_state) + (f * stilde)\r\n",
        "\r\n",
        "            h_state = h\r\n",
        "\r\n",
        "            h_list.append(h_state)\r\n",
        "\r\n",
        "        hidden_outputs = tf.stack(h_list, axis=1)\r\n",
        "\r\n",
        "        return hidden_outputs[:, -1, :]\r\n",
        "\r\n",
        "class MGU(tf.keras.Model):\r\n",
        "    def __init__(self,units,num_classes):\r\n",
        "        super(MGU,self).__init__()\r\n",
        "        self.units = units\r\n",
        "        self.mgu = BasicMGU(units)\r\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes)\r\n",
        "\r\n",
        "    def call(self,inputs,training=None,mask=None):\r\n",
        "        h = self.mgu(inputs)\r\n",
        "        output = self.classifier(h)\r\n",
        "\r\n",
        "        with tf.device('/cpu:0'):\r\n",
        "            output = tf.nn.softmax(output)\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vQBb0vF3rkc"
      },
      "source": [
        "# Parameters\r\n",
        "num_inputs = 28\r\n",
        "num_timesteps = 28\r\n",
        "num_hiddenunits = 128\r\n",
        "num_classes = 10\r\n",
        "\r\n",
        "num_epochs = 10\r\n",
        "batch_size = 64\r\n",
        "learning_rate = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSm7QSjb3sUj"
      },
      "source": [
        "def loss(model,X,Y):\r\n",
        "    logits = model(tf.convert_to_tensor(X, tf.float32))\r\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZK6wPIv3up1"
      },
      "source": [
        "def accuracy(y_pred, y_true):\r\n",
        "    predicted = tf.nn.softmax(y_pred)\r\n",
        "    correct_prediction = tf.equal(tf.argmax(predicted,1), tf.argmax(y_true,1))\r\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpJBgFY430vU"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\r\n",
        "gradients = tfe.implicit_value_and_gradients(loss)\r\n",
        "\r\n",
        "#model = LSTM(num_hiddenunits, num_classes)\r\n",
        "#model = GRU(num_hiddenunits, num_classes)\r\n",
        "model = MGU(num_hiddenunits, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-ScDmHs31bl"
      },
      "source": [
        "#data = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n",
        "data = notMNIST()\r\n",
        "\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((data.train.images, data.train.labels)).map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\r\n",
        "          .shuffle(buffer_size=1000)\\\r\n",
        "          .batch(batch_size=64)\\\r\n",
        "\r\n",
        "X_test = np.array(data.test.images[:]).reshape([-1, 28, 28])\r\n",
        "y_test = data.test.labels[:]\r\n",
        "\r\n",
        "time_start = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sngW5e_2tcp"
      },
      "source": [
        "######################################### Training Procedure ###############################################\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    train_accuracy = []\r\n",
        "    test_accuracy = []\r\n",
        "    print(\"Epoch: {}\".format(epoch+1))\r\n",
        "    total_loss = tfe.Variable(0, dtype = tf.float32)\r\n",
        "    for step, (image_batch, label_batch) in enumerate(tfe.Iterator(train_ds)):\r\n",
        "        image_batch, label_batch = data.train.next_batch(batch_size)\r\n",
        "        image_batch = np.array(image_batch).reshape([batch_size, 28, 28])\r\n",
        "        loss, grads_and_vars = gradients(model, image_batch, label_batch)\r\n",
        "        optimizer.apply_gradients(grads_and_vars)\r\n",
        "        #total_loss += loss\r\n",
        "        if (step%50 == 0):\r\n",
        "            print(\"Step: {} Loss: {:.4f}\".format(step, loss))\r\n",
        "            train_images = model(tf.convert_to_tensor(image_batch, tf.float32))\r\n",
        "            train_accuracy.append(accuracy(train_images,label_batch))\r\n",
        "            test_images = model(tf.convert_to_tensor(X_test, tf.float32))\r\n",
        "            test_accuracy.append(accuracy(test_images, y_test))\r\n",
        "            \r\n",
        "            \r\n",
        "plt.plot(train_accuracy, label ='train')\r\n",
        "plt.ylim(0,1)\r\n",
        "plt.plot(test_accuracy, label = 'test')\r\n",
        "plt.ylim(0,1)\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.title('Training vs Test Curve')\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print(\"Loss:{:.4f}\".format(loss))\r\n",
        "\r\n",
        "test_images = model(tf.convert_to_tensor(X_test, tf.float32))\r\n",
        "print(\"Test Accuracy :{:.4f}\".format(accuracy(test_images, y_test)))\r\n",
        "\r\n",
        "time_taken = time.time() - time_start\r\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}